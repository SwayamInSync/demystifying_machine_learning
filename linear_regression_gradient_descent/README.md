# Linear Regression using Gradient Descent

<img src="https://images.unsplash.com/photo-1543286386-2e659306cd6c?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2070&q=80" style="zoom:50%;" />



## Overview

This is the second article of **Demystifying Machine Learning** series, fankly it is basically the ***sequel*** of our previous article where we explained [**Linear Regression using Normal equation**](https://swayam-blog.hashnode.dev/linear-regression-using-normal-equation). In this article we'll be exploring another optimizing algorithm known as **Gradient Descent**, how it works, what is a cost function, mathematics behined gradient descent, Python implementation, Regularization and some extra topics like polynomial regression and using regularized polynomial regression.

## How Gradient Descent works (Intuition)

